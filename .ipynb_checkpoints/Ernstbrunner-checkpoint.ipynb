{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attr_3': ['democrat',\n",
      "            {'attr_9': [{'attr_2': ['republican',\n",
      "                                    {'attr_1': ['democrat', 'republican']}]},\n",
      "                        {'attr_8': ['democrat', 'republican']}]}]}\n",
      "{'attr_3': ['democrat',\n",
      "            {'attr_2': [{'attr_1': [{'attr_7': ['republican',\n",
      "                                                {'attr_6': ['democrat',\n",
      "                                                            'republican']}]},\n",
      "                                    'republican']},\n",
      "                        'republican']}]}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "# class definitions\n",
    "\n",
    "class Tree:\n",
    "\n",
    "    edge_counter = 0\n",
    "    \n",
    "    def entropy(X):\n",
    "\n",
    "        # compute probability distribution of attribute X\n",
    "        unique, counts = np.unique(X, return_counts=True)\n",
    "        px = counts / len(X)\n",
    "\n",
    "        # return the log base 2 entropy\n",
    "        return -np.dot(px, np.log2(px))\n",
    "\n",
    "    def compute_gain(S, i):\n",
    "        \"\"\" computes the information gain H(S) - H(S|i)\n",
    "        \"\"\"\n",
    "        H_S = entropy(S)\n",
    "\n",
    "        # compute the conditional entropy H(S|i)\n",
    "        avg_H = 0\n",
    "        unique, counts = np.unique(i, return_counts=True)\n",
    "        s = 1 / len(S)\n",
    "        for j in range(len(unique)):\n",
    "            idx = np.where(unique[j] == i)[0]\n",
    "            avg_H += len(idx) * s * entropy(S[idx])\n",
    "\n",
    "        return H_S - avg_H\n",
    "\n",
    "    def majority_leaf(data):\n",
    "        unique, counts = np.unique(data[:,0], return_counts=True)\n",
    "        return unique[np.argmax(counts)]\n",
    "    \n",
    "    def ID3(self, d, n, data):\n",
    "        \"\"\" this function implements the ID3 algorithm based on the\n",
    "            definition given in Lab2.\n",
    "        input: \n",
    "            d := maximum depth of the tree, will be reduced on each level\n",
    "            n := the maximum number of nodes. The static edge_counter variable\n",
    "            shall not exceed the number of n - 1 edges.\n",
    "        data := 2d numpy array {columns: features} X {rows: samples}\n",
    "   \n",
    "        output:\n",
    "            tree := the binary decision tree for the dataset\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "        # return a leaf in case of\n",
    "        #   - uniform labels\n",
    "        #   - maximal depth\n",
    "        #   - maximum number of n nodes with n-1 edges reached\n",
    "        if (len(np.unique(data[:,0])) == 1) or (d == 1) or (self.edge_counter >= n-1):\n",
    "            return majority_leaf(data)\n",
    "        \n",
    "        # else investigate node further\n",
    "        else:\n",
    "            \n",
    "            # return a leaf if no more attributes are left \n",
    "            if data.shape[1] == 1:\n",
    "                return majority_leaf(data)\n",
    "            \n",
    "            # compute the information gain IG for each attribute\n",
    "            gains = [compute_gain(data[:,0], data[:,x]) for x in range(1, data.shape[1])]    \n",
    "            \n",
    "            # get the attribute with highest IG\n",
    "            split_column = np.argmax(gains)\n",
    "            \n",
    "            # compute separate row indices for 'y' and 'n'\n",
    "            # according to the attribute with highest IG\n",
    "            yea_idx = np.where(data[:,split_column] == 'y')[0]\n",
    "            nea_idx = np.where(data[:,split_column] == 'n')[0]\n",
    "\n",
    "            # return a leaf if samples are empty\n",
    "            if len(nea_idx) == 0:\n",
    "                return majority_leaf(data)\n",
    "            if len(yea_idx) == 0:\n",
    "                return majority_leaf(data)\n",
    "            \n",
    "            # split the data and remove the attribute with highest IG\n",
    "            data_yea = np.delete(data[yea_idx,:], split_column, 1)\n",
    "            data_nea = np.delete(data[nea_idx,:], split_column, 1)\n",
    "\n",
    "            # label the decision node\n",
    "            decision = 'attr_' + str(split_column)\n",
    "\n",
    "            # instantiate sub-tree for the decision node\n",
    "            tree = {decision: []}\n",
    "            \n",
    "            # increase the edge counter and reduce the max depth\n",
    "            self.edge_counter +=2\n",
    "            d -= 1\n",
    "\n",
    "            yea = self.ID3(d, n, data_yea)\n",
    "            nea = self.ID3(d, n, data_nea)\n",
    "\n",
    "            if yea == nea:\n",
    "                tree = yea\n",
    "            else:\n",
    "                tree[decision].append(yea)\n",
    "                tree[decision].append(nea)\n",
    "            return tree\n",
    "\n",
    "# function definitions\n",
    "\n",
    "def load_data(fname):\n",
    "    data = np.genfromtxt(fname, delimiter=',', dtype=str)\n",
    "        \n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            if data[i][j] == '?':\n",
    "                \n",
    "                # for each element set a random choice\n",
    "                data[i][j] = np.random.choice(['y', 'n'])\n",
    "    return data\n",
    "        \n",
    "def split_data(data, split):\n",
    "    \n",
    "    # create permuted indices for training and test sets\n",
    "    perm = np.random.permutation(np.indices((len(data),))[0])\n",
    "    \n",
    "    # calculate the number of rows according to the given percentage\n",
    "    nrows = int(split * len(data))\n",
    "    training_set = data[perm[:nrows]]\n",
    "    test_set = data[perm[nrows:]]\n",
    "    return training_set, test_set\n",
    "\n",
    "def learning_curve(d, n, training_set, test_set):\n",
    "    # you will probably need additional helper functions\n",
    "    return plot\n",
    "\n",
    "# main\n",
    "\n",
    "data = load_data('house-votes-84.data')\n",
    "\n",
    "k = np.array([1,32,4,5])\n",
    "\n",
    "# set seed before splitting the data\n",
    "np.random.seed(666)\n",
    "tr_s, te_s = split_data(data, .7)\n",
    "\n",
    "n = 20\n",
    "d = 16\n",
    "\n",
    "Tree = Tree()\n",
    "\n",
    "pprint(Tree.ID3(d, n, tr_s))\n",
    "\n",
    "Tree.edge_counter = 0\n",
    "pprint(Tree.ID3(d, n, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "idea for building a tree: https://github.com/SebastianMantey/Decision-Tree-from-Scratch/blob/master/notebooks/handling%20continuous%20and%20categorical%20variables.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
