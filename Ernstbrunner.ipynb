{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# class definition\n",
    "\n",
    "class ID3Model:\n",
    "\n",
    "    edge_counter = 0\n",
    "    tree = None\n",
    "        \n",
    "    def ID3(self, d, n, data):\n",
    "        \"\"\" description: this function implements the ID3 algorithm based on the\n",
    "            definition given in the assignment.\n",
    "        input: \n",
    "            d := maximum depth of the tree, will be reduced on each level\n",
    "            n := the maximum number of nodes. The static edge_counter variable\n",
    "            shall not exceed the number of n - 1 edges.\n",
    "            data := 2d numpy array {columns: features} X {rows: samples}\n",
    "        output:\n",
    "            tree := the binary decision tree for the dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        # return a leaf in case of\n",
    "        #   - uniform labels\n",
    "        #   - maximal depth\n",
    "        #   - maximum number of n nodes with n-1 edges\n",
    "        #   - no more attributes \n",
    "        if (len(np.unique(data[:,0])) <= 1) or (d == 1) or (self.edge_counter >= n-1) or (data.shape[1] == 1):\n",
    "            return self.majority_leaf(data)\n",
    "        \n",
    "        # else investigate node further\n",
    "        else:\n",
    "            \n",
    "            # compute the information gain IG for each attribute\n",
    "            gains = [self.compute_gain(data[:,0], data[:,x]) for x in range(1, data.shape[1])]    \n",
    "            \n",
    "            # get the attribute with highest IG\n",
    "            split_column = np.argmax(gains)\n",
    "            \n",
    "            # separate row indices for 'y' and 'n' according\n",
    "            # to the attribute with highest IG\n",
    "            yea_idx = np.where(data[:,split_column] == 'y')[0]\n",
    "            nea_idx = np.where(data[:,split_column] == 'n')[0]\n",
    "\n",
    "            # return a leaf if samples are empty\n",
    "            if len(nea_idx) == 0:\n",
    "                return self.majority_leaf(data)\n",
    "            if len(yea_idx) == 0:\n",
    "                return self.majority_leaf(data)\n",
    "            \n",
    "            # split the data and remove the attribute with highest IG\n",
    "            data_yea = np.delete(data[yea_idx,:], split_column, 1)\n",
    "            data_nea = np.delete(data[nea_idx,:], split_column, 1)\n",
    "\n",
    "            # label the decision node\n",
    "            decision = str(split_column)\n",
    "\n",
    "            # initialize sub-tree for the decision node\n",
    "            tree = {decision: []}\n",
    "            \n",
    "            # increase the edge counter and reduce the max depth\n",
    "            self.edge_counter +=2\n",
    "            d -= 1\n",
    "            \n",
    "            # recursive calls on the split data\n",
    "            yea = self.ID3(d, n, data_yea)\n",
    "            nea = self.ID3(d, n, data_nea)\n",
    "            \n",
    "            # in case of equality return any leaf\n",
    "            if yea == nea:\n",
    "                tree = yea\n",
    "            \n",
    "            # else append 'yes' branch and 'no' branch\n",
    "            else:\n",
    "                tree[decision].append(yea)\n",
    "                tree[decision].append(nea)\n",
    "            return tree\n",
    "\n",
    "    def compute_gain(self, S, i):\n",
    "        \"\"\" description: compute the information gain H(S) - H(S|i)\n",
    "        \"\"\"\n",
    "        H_S = self.entropy(S)\n",
    "\n",
    "        # compute the conditional entropy H(S|i)\n",
    "        avg_H = 0\n",
    "        unique, counts = np.unique(i, return_counts=True)\n",
    "        s = 1 / len(S)\n",
    "        for j in range(len(unique)):\n",
    "            idx = np.where(unique[j] == i)[0]\n",
    "            avg_H += len(idx) * s * self.entropy(S[idx])\n",
    "\n",
    "        return H_S - avg_H\n",
    "    \n",
    "    def entropy(self, X):\n",
    "\n",
    "        # compute probability distribution of attribute X\n",
    "        unique, counts = np.unique(X, return_counts=True)\n",
    "        px = counts / len(X)\n",
    "\n",
    "        # return the log base 2 entropy\n",
    "        return -np.dot(px, np.log2(px))\n",
    "    \n",
    "    def majority_leaf(self, data):\n",
    "        \"\"\" description: compute and return the most common label for a leaf\n",
    "        \"\"\"\n",
    "        unique, counts = np.unique(data[:,0], return_counts=True)\n",
    "        return unique[np.argmax(counts)]    \n",
    "    \n",
    "    def fit(self, d, n, data):\n",
    "        \"\"\" description: train a tree based on the given data\n",
    "        input: \n",
    "            d := maximum depth of the tree, will be reduced on each level\n",
    "            n := the maximum number of nodes. The static edge_counter variable\n",
    "            shall not exceed the number of n - 1 edges.\n",
    "            data := 2d numpy array {columns: features} X {rows: samples}\n",
    "        \"\"\"\n",
    "        self.edge_counter = 0\n",
    "        self.tree = self.ID3(d, n, data)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def evaluate(self, data):\n",
    "        \"\"\" description: evaluate the model \n",
    "            for the given data\n",
    "        \"\"\"\n",
    "        # tree not built\n",
    "        if self.tree is None:\n",
    "            print('no tree to evaluate')\n",
    "            return np.nan\n",
    "        \n",
    "        fail_counter = 0\n",
    "        \n",
    "        # evaluate all samples from the data\n",
    "        for x in range(data.shape[0]):\n",
    "            sample = data[x]\n",
    "            result = self.classify_sample(sample, self.tree)\n",
    "            if result != sample[0]:\n",
    "                fail_counter +=1\n",
    "\n",
    "        return fail_counter / data.shape[0]\n",
    "    \n",
    "    def classify_sample(self, sample, tree):\n",
    "        \"\"\" description: classify a sample with the given model\n",
    "        \"\"\"\n",
    "        if (tree == 'democrat') or (tree == 'republican'):\n",
    "            return tree\n",
    "        \n",
    "        decision = list(tree.keys())[0]\n",
    "        attribute = int(decision)\n",
    "\n",
    "        if sample[attribute] == 'y':\n",
    "            result = tree[decision][0]\n",
    "        if sample[attribute] == 'n':\n",
    "            result = tree[decision][1]\n",
    "\n",
    "        if (result == 'democrat') or (result == 'republican'):\n",
    "            return result\n",
    "\n",
    "        else:\n",
    "            return self.classify_sample(sample, result)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "# function definitions\n",
    "\n",
    "def load_data(fname):\n",
    "\n",
    "    data = np.genfromtxt(fname, delimiter=',', dtype=str)\n",
    "        \n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            if data[i][j] == '?':\n",
    "                \n",
    "                # for each element set a random choice\n",
    "                data[i][j] = np.random.choice(['y', 'n'])\n",
    "                \n",
    "    return data\n",
    "        \n",
    "\n",
    "def split_data(data, split):\n",
    "    \n",
    "    # create permuted indices for training and test sets\n",
    "    perm = np.random.permutation(np.indices((len(data),))[0])\n",
    "    \n",
    "    # calculate the number of rows according to the given percentage\n",
    "    nrows = int(split * len(data))\n",
    "    training_set = data[perm[:nrows]]\n",
    "    test_set = data[perm[nrows:]]\n",
    "    return training_set, test_set\n",
    "\n",
    "\n",
    "def learning_curve(d, n, training_set, test_set, increment=10):\n",
    "    \"\"\" description: train and evaluate training and test data\n",
    "        for different sizes of the test data and plot the results.\n",
    "        input: \n",
    "            d := maximum depth of the tree, will be reduced on each level\n",
    "            n := the maximum number of nodes. The static edge_counter variable\n",
    "            shall not exceed the number of n - 1 edges.\n",
    "            training_set := 2d numpy array {columns: features} X {rows: samples}\n",
    "            test_set := 2d numpy array {columns: features} X {rows: samples}\n",
    "            increment := the parameter 'r' as is defined in the assignment\n",
    "    \"\"\"\n",
    "    # define min. data (==10) and max. data (==#rows of the training set)\n",
    "    start = 10\n",
    "    end = training_set.shape[0]\n",
    "    m = range(start, end, increment)\n",
    "    \n",
    "    training_error = list(m)\n",
    "    test_error = list(m)\n",
    "    \n",
    "    # istantiate the classifier\n",
    "    t = ID3Model()\n",
    "    \n",
    "    for x in m:\n",
    "        \n",
    "        i = int((x - 10) / increment)\n",
    "        \n",
    "        # build tree and evaluate training and test errors\n",
    "        training_error[i] = t.fit(d, n, training_set[0:x]).evaluate(training_set)\n",
    "        test_error[i] = t.evaluate(test_set)\n",
    "    \n",
    "    # setup plot\n",
    "    plt.figure()\n",
    "    plt.xlabel('training set size')\n",
    "    plt.ylabel('training / test error')\n",
    "    plt.ylim([0, .6])\n",
    "    plt.title('ID3(d:{},  n:{})'.format(d, n))\n",
    "\n",
    "    # plot results\n",
    "    plt.plot(m, training_error, '-.')\n",
    "    plt.plot(m, test_error, 'r-')\n",
    "    plt.legend(['training error', 'test error'])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# main program\n",
    "\n",
    "# set seed before splitting the data for fixed 'random' values\n",
    "np.random.seed(6666)\n",
    "\n",
    "# load data\n",
    "data = load_data('house-votes-84.data')\n",
    "\n",
    "# split the data into training and test set\n",
    "training_set, test_set = split_data(data, .7)\n",
    "\n",
    "# initialize given parameters from the assignment\n",
    "d = np.array([7, 4, 16, 7, 7])\n",
    "n = np.array([40, 40, 40, 20, 130])\n",
    "\n",
    "# call the learning_curve function for all 5 parameter constellations\n",
    "for x in range(0,5):\n",
    "    learning_curve(d[x], n[x], training_set, test_set, 1)\n",
    "    #plt.savefig('../plots/fig2'+ str(x) +'.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "idea for building and evaluating a tree: https://github.com/SebastianMantey/Decision-Tree-from-Scratch/blob/master/notebooks/handling%20continuous%20and%20categorical%20variables.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
